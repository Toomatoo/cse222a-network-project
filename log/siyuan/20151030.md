# 10/30/2015

## Setup Mininet

// Install Mininet Environment

Install from source code

// Customize Topologies

sudo mn --custom ~/mininet/custom/topo-2sw-2host.py --topo mytopo --test pingall

// Set MAC and IP addr to a small number

sudo mn --mac

// Open term for host and switch

sudo mn -x

mininet> xterm h1...


## Use OpenFlow on Mininet

// Example a mininet with a Openflow switch

`sudo mn --topo single,3 --mac --switch ovsk --controller remote`

After this command, I can monitor the flow on my bash shell.

Then I take a ping test: 

`mininet> h1 ping -c3 h2`

Now there is no flow on the mininet, I need to open flow for it.

`$ ovs-ofctl add-flow s1 in_port=1, actions=output:2`

Ping test again, we can receive succuss. And,

`$ ovs-ofctl dump-flows s1`

we can see the flow table.

    127.0.0.1:6633 is the controller
    127.0.0.1:<6633+n> is port for every openflow switch.


## Hadoop

Set masters and slaves IP, but how to use a same one: SSH? Direct network access?

Maybe set a masters file by myself

## Configuration

http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/

I can config a hadoop on a single node with this blog.

The process consists of adding a user and configing SSH, Modifying XML files to setup hadoop, start single node, test MapReduce on the hadoop.

There are some problems appearing in the process:

1. copyFromLocal: copy from hdfs, so you can not find the exact file in the computer. They are in the hdfs, use dfs tool to take actions on it. http://stackoverflow.com/questions/19912847/copyfromlocal-user-hduser-gutenberg-no-such-file-or-directory
2. Hadoop MapReduce Example is under the directory share/mapreduce/...


## Question

Can not access network through Mininet
